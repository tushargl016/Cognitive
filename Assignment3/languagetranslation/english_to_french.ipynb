{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = []\n",
    "french = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = open('english.txt', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "french = open('french.txt', encoding='latin-1').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = 0\n",
    "with open('english.txt','r') as f:\n",
    "    for line in f:\n",
    "        num_lines +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137860"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_line = 0\n",
    "with open('english.txt','r') as f:\n",
    "    for line in f:\n",
    "        num_line +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137860"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "# import problem_unittests as tests\n",
    "\n",
    "source_path = 'english.txt'\n",
    "target_path = 'french.txt'\n",
    "\n",
    "english_chars = set()\n",
    "french_chars = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_texts = []\n",
    "target_texts = []\n",
    "with open(source_path, 'r', encoding='utf-8') as f:\n",
    "    input_lines = f.read().split('\\n')\n",
    "for input_line in input_lines[: min(sample_size, len(input_lines) - 1)]:\n",
    "    english_text = input_line.split()\n",
    "    english_texts.append(english_text)\n",
    "    for char in english_text:\n",
    "        if char not in english_chars:\n",
    "            english_chars.add(char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(target_path, 'r', encoding='latin-1') as f:\n",
    "    target_lines = f.read().split('\\n')\n",
    "for target_line in target_lines[: min(sample_size, len(target_lines) - 1)]:\n",
    "    target_text = target_line.split()\n",
    "    target_texts.append(target_text)\n",
    "    for char in target_text:\n",
    "        if char not in french_chars:\n",
    "            french_chars.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_chars= sorted(list(english_chars))\n",
    "french_chars = sorted(list(french_chars))\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(english_chars)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(french_chars)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 0,\n",
       " '-': 1,\n",
       " '.': 2,\n",
       " '?': 3,\n",
       " 'a': 4,\n",
       " 'agréable': 5,\n",
       " 'aimait': 6,\n",
       " 'aime': 7,\n",
       " 'aiment': 8,\n",
       " 'aimez': 9,\n",
       " 'aimons': 10,\n",
       " 'aimé': 11,\n",
       " 'aimé.': 12,\n",
       " 'aimée': 13,\n",
       " 'aimés': 14,\n",
       " 'aller': 15,\n",
       " 'allons': 16,\n",
       " 'allé': 17,\n",
       " 'amusant': 18,\n",
       " 'animal': 19,\n",
       " 'animaux': 20,\n",
       " 'août': 21,\n",
       " 'au': 22,\n",
       " 'automne': 23,\n",
       " 'automobile': 24,\n",
       " 'aux': 25,\n",
       " 'avril': 26,\n",
       " 'banane': 27,\n",
       " 'bananes': 28,\n",
       " 'beau': 29,\n",
       " 'belle': 30,\n",
       " 'blanc': 31,\n",
       " 'blanche': 32,\n",
       " 'bleu': 33,\n",
       " 'bleue': 34,\n",
       " 'brillant': 35,\n",
       " 'brillante': 36,\n",
       " 'california': 37,\n",
       " 'californie': 38,\n",
       " 'calme': 39,\n",
       " 'camion': 40,\n",
       " 'ce': 41,\n",
       " 'cet': 42,\n",
       " 'cette': 43,\n",
       " 'chat': 44,\n",
       " 'chaud': 45,\n",
       " 'chaude': 46,\n",
       " 'chaux': 47,\n",
       " 'cher': 48,\n",
       " 'cheval': 49,\n",
       " 'chevaux': 50,\n",
       " 'chien': 51,\n",
       " 'chiens': 52,\n",
       " 'chine': 53,\n",
       " 'chinois': 54,\n",
       " 'citron': 55,\n",
       " 'citrons': 56,\n",
       " 'clémentes': 57,\n",
       " 'comme': 58,\n",
       " 'comment': 59,\n",
       " 'conduisait': 60,\n",
       " 'conduit': 61,\n",
       " 'cours': 62,\n",
       " \"d'\": 63,\n",
       " 'dans': 64,\n",
       " 'de': 65,\n",
       " 'dernier': 66,\n",
       " 'des': 67,\n",
       " 'difficile': 68,\n",
       " 'doux': 69,\n",
       " 'décembre': 70,\n",
       " 'détendre': 71,\n",
       " 'détestait': 72,\n",
       " 'déteste': 73,\n",
       " 'détestons': 74,\n",
       " 'eiffel': 75,\n",
       " 'elle': 76,\n",
       " 'en': 77,\n",
       " 'enneigée': 78,\n",
       " 'entre': 79,\n",
       " 'espagnol': 80,\n",
       " 'est': 81,\n",
       " 'est-ce': 82,\n",
       " 'et': 83,\n",
       " 'fait': 84,\n",
       " 'favori': 85,\n",
       " 'fraise': 86,\n",
       " 'fraises': 87,\n",
       " 'france': 88,\n",
       " 'frisquet': 89,\n",
       " 'froid': 90,\n",
       " 'fruit': 91,\n",
       " 'fruits': 92,\n",
       " 'février': 93,\n",
       " 'gel': 94,\n",
       " 'glaciales': 95,\n",
       " 'grand': 96,\n",
       " 'grande': 97,\n",
       " 'gros': 98,\n",
       " 'grosse': 99,\n",
       " 'gèle': 100,\n",
       " 'généralement': 101,\n",
       " 'habituellement': 102,\n",
       " 'hiver': 103,\n",
       " 'humide': 104,\n",
       " 'i': 105,\n",
       " 'il': 106,\n",
       " 'ils': 107,\n",
       " 'inde': 108,\n",
       " \"j'aime\": 109,\n",
       " 'jamais': 110,\n",
       " 'janvier': 111,\n",
       " 'jaune': 112,\n",
       " 'je': 113,\n",
       " 'jersey': 114,\n",
       " 'juillet': 115,\n",
       " 'juin': 116,\n",
       " \"l'\": 117,\n",
       " \"l'automne\": 118,\n",
       " \"l'automobile\": 119,\n",
       " \"l'oiseau\": 120,\n",
       " \"l'orange\": 121,\n",
       " \"l'ours\": 122,\n",
       " \"l'éléphant\": 123,\n",
       " \"l'épicerie\": 124,\n",
       " 'la': 125,\n",
       " 'lapin': 126,\n",
       " 'le': 127,\n",
       " 'les': 128,\n",
       " 'leur': 129,\n",
       " 'leurs': 130,\n",
       " 'lion': 131,\n",
       " 'lui': 132,\n",
       " 'légère': 133,\n",
       " 'magnifique': 134,\n",
       " 'mai': 135,\n",
       " 'maillot': 136,\n",
       " 'mais': 137,\n",
       " 'mangue': 138,\n",
       " 'mangues': 139,\n",
       " 'mars': 140,\n",
       " 'merveilleux': 141,\n",
       " 'mes': 142,\n",
       " 'moins': 143,\n",
       " 'mois': 144,\n",
       " 'mon': 145,\n",
       " 'monde': 146,\n",
       " 'mouillée': 147,\n",
       " \"n'aimait\": 148,\n",
       " \"n'aime\": 149,\n",
       " \"n'aiment\": 150,\n",
       " \"n'aimez\": 151,\n",
       " \"n'aimons\": 152,\n",
       " 'ne': 153,\n",
       " 'neige': 154,\n",
       " 'neigeux': 155,\n",
       " 'new': 156,\n",
       " 'noir': 157,\n",
       " 'noire': 158,\n",
       " 'nos': 159,\n",
       " 'notre': 160,\n",
       " 'nous': 161,\n",
       " 'nouveau': 162,\n",
       " 'nouvelle': 163,\n",
       " 'novembre': 164,\n",
       " 'occupé': 165,\n",
       " 'occupée': 166,\n",
       " 'octobre': 167,\n",
       " 'oiseau': 168,\n",
       " 'oranges': 169,\n",
       " 'ours': 170,\n",
       " 'où': 171,\n",
       " 'pamplemousse': 172,\n",
       " 'pamplemousses': 173,\n",
       " 'parfois': 174,\n",
       " 'paris': 175,\n",
       " 'pas': 176,\n",
       " 'pendant': 177,\n",
       " 'pense': 178,\n",
       " 'petit': 179,\n",
       " 'petite': 180,\n",
       " 'plaît': 181,\n",
       " 'pleut': 182,\n",
       " 'pluie': 183,\n",
       " 'pluies': 184,\n",
       " 'plus': 185,\n",
       " 'pluvieux': 186,\n",
       " 'poire': 187,\n",
       " 'poires': 188,\n",
       " 'pomme': 189,\n",
       " 'pommes': 190,\n",
       " 'porcelaine': 191,\n",
       " 'portugais': 192,\n",
       " 'pourquoi': 193,\n",
       " 'pourraient': 194,\n",
       " 'pourrait': 195,\n",
       " 'printemps': 196,\n",
       " 'prochain': 197,\n",
       " 'proches': 198,\n",
       " 'préféré': 199,\n",
       " 'préféré.': 200,\n",
       " 'préférée': 201,\n",
       " 'préférés': 202,\n",
       " 'prévoient': 203,\n",
       " 'prévois': 204,\n",
       " 'prévoit': 205,\n",
       " 'prévoyons': 206,\n",
       " 'pêche': 207,\n",
       " 'pêches': 208,\n",
       " \"qu'il\": 209,\n",
       " 'quand': 210,\n",
       " 'que': 211,\n",
       " 'raisin': 212,\n",
       " 'raisins': 213,\n",
       " 'redouté': 214,\n",
       " 'redoutés': 215,\n",
       " 'relaxant': 216,\n",
       " 'rendre': 217,\n",
       " 'requin': 218,\n",
       " 'requins': 219,\n",
       " 'rouge': 220,\n",
       " 'rouillé': 221,\n",
       " 'rouillée': 222,\n",
       " 'se': 223,\n",
       " 'sec': 224,\n",
       " 'septembre': 225,\n",
       " 'ses': 226,\n",
       " 'singe': 227,\n",
       " 'son': 228,\n",
       " 'sont': 229,\n",
       " 'souris': 230,\n",
       " 'sèche': 231,\n",
       " 'temps': 232,\n",
       " 'tour': 233,\n",
       " 'traduction': 234,\n",
       " 'traduire': 235,\n",
       " 'tranquille': 236,\n",
       " 'trop': 237,\n",
       " 'un': 238,\n",
       " 'une': 239,\n",
       " 'va': 240,\n",
       " 'vert': 241,\n",
       " 'verte': 242,\n",
       " 'verts': 243,\n",
       " 'vieille': 244,\n",
       " 'vieux': 245,\n",
       " 'visiter': 246,\n",
       " 'voiture': 247,\n",
       " 'volant': 248,\n",
       " 'vont': 249,\n",
       " 'vos': 250,\n",
       " 'votre': 251,\n",
       " 'voulait': 252,\n",
       " 'vous': 253,\n",
       " 'vu': 254,\n",
       " 'États-unis': 255,\n",
       " 'à': 256,\n",
       " 'éléphants': 257,\n",
       " 'étaient': 258,\n",
       " 'était': 259,\n",
       " 'états-unis': 260,\n",
       " 'été': 261}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros((len(english_texts), max([len(txt) for txt in english_texts]), len(english_chars)),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros((len(english_texts),  max([len(txt) for txt in target_texts]), len(french_chars)),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros((len(english_texts),  max([len(txt) for txt in target_texts]), len(french_chars)),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (english_text, target_text) in enumerate(zip(english_texts, target_texts)):\n",
    "    for t, char in enumerate(english_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text): \n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, len(english_chars)))\n",
    "encoder = LSTM(256 , return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,len(french_chars)))\n",
    "\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(french_chars), activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.5743 - val_loss: 0.6905\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.5143 - val_loss: 0.6871\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.5075 - val_loss: 0.7120\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.4951 - val_loss: 0.6918\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.4894 - val_loss: 0.6859\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.4916 - val_loss: 0.7012\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 1s 981us/step - loss: 0.4702 - val_loss: 0.6828\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.4694 - val_loss: 0.6813\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.4588 - val_loss: 0.6858\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.4502 - val_loss: 0.6707\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.4451 - val_loss: 0.6858\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.4409 - val_loss: 0.6778\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.4296 - val_loss: 0.6571\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.4203 - val_loss: 0.6695\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.4223 - val_loss: 0.6566\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.4125 - val_loss: 0.6689\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.3999 - val_loss: 0.6479\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 1s 981us/step - loss: 0.3911 - val_loss: 0.6541\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.3853 - val_loss: 0.6617\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.3862 - val_loss: 0.6686\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.3657 - val_loss: 0.6587\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.3708 - val_loss: 0.6571\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.3583 - val_loss: 0.6472\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.3477 - val_loss: 0.6566\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.3458 - val_loss: 0.6671\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.3395 - val_loss: 0.6593\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.3286 - val_loss: 0.6698\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.3174 - val_loss: 0.6778\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.3215 - val_loss: 0.6606\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.3071 - val_loss: 0.6726\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.2955 - val_loss: 0.6807\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.2955 - val_loss: 0.6812\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.2845 - val_loss: 0.6778\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.2780 - val_loss: 0.6656\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.2733 - val_loss: 0.6725\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.2605 - val_loss: 0.6743\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.2589 - val_loss: 0.6732\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.2482 - val_loss: 0.6732\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.2386 - val_loss: 0.6845\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 1s 980us/step - loss: 0.2364 - val_loss: 0.6878\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.2270 - val_loss: 0.6821\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.2106 - val_loss: 0.6846\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.2212 - val_loss: 0.6893\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.2006 - val_loss: 0.6835\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.2013 - val_loss: 0.6772\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.1805 - val_loss: 0.6936\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.1965 - val_loss: 0.6919\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.1685 - val_loss: 0.7025\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.1692 - val_loss: 0.7062\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.1666 - val_loss: 0.7098\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.1609 - val_loss: 0.6835\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.1503 - val_loss: 0.7045\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.1407 - val_loss: 0.7093\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.1368 - val_loss: 0.7245\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.1344 - val_loss: 0.7273\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.1216 - val_loss: 0.7145\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.1244 - val_loss: 0.7345\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.1113 - val_loss: 0.7290\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.1145 - val_loss: 0.7239\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.1092 - val_loss: 0.7128\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.1007 - val_loss: 0.7227\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0951 - val_loss: 0.7302\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0910 - val_loss: 0.7370\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0869 - val_loss: 0.7383\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0888 - val_loss: 0.7502\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0791 - val_loss: 0.7550\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0721 - val_loss: 0.7661\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0825 - val_loss: 0.7785\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0638 - val_loss: 0.7534\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0641 - val_loss: 0.7788\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0699 - val_loss: 0.7784\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.0527 - val_loss: 0.7637\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 1s 981us/step - loss: 0.0610 - val_loss: 0.8092\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0599 - val_loss: 0.7821\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0483 - val_loss: 0.7750\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0579 - val_loss: 0.7900\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0428 - val_loss: 0.8282\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0555 - val_loss: 0.7918\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 1s 984us/step - loss: 0.0442 - val_loss: 0.8136\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0418 - val_loss: 0.8148\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 1s 989us/step - loss: 0.0467 - val_loss: 0.7963\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0425 - val_loss: 0.8338\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0376 - val_loss: 0.7974\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0378 - val_loss: 0.8244\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0354 - val_loss: 0.8218\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0325 - val_loss: 0.8526\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0441 - val_loss: 0.8112\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0276 - val_loss: 0.9020\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0432 - val_loss: 0.8243\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0223 - val_loss: 0.8261\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0389 - val_loss: 0.8528\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.0290 - val_loss: 0.8362\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0228 - val_loss: 0.8819\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0409 - val_loss: 0.8368\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0185 - val_loss: 0.8598\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0273 - val_loss: 0.8573\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0228 - val_loss: 0.9215\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0379 - val_loss: 0.8564\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0151 - val_loss: 0.8633\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.0252 - val_loss: 0.8803\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0348 - val_loss: 0.8589\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 1s 990us/step - loss: 0.0191 - val_loss: 0.9030\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0260 - val_loss: 0.8640\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0140 - val_loss: 0.8706\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.0317 - val_loss: 0.8905\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0215 - val_loss: 0.9315\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0224 - val_loss: 0.8834\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0118 - val_loss: 0.8793\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0338 - val_loss: 0.8933\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0148 - val_loss: 0.9194\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 1s 981us/step - loss: 0.0187 - val_loss: 0.8763\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0106 - val_loss: 0.9001\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0280 - val_loss: 0.8907\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0124 - val_loss: 0.9138\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.0198 - val_loss: 0.9204\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0186 - val_loss: 0.9049\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0091 - val_loss: 0.9122\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0114 - val_loss: 0.8949\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0248 - val_loss: 0.9366\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0154 - val_loss: 0.9163\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0093 - val_loss: 0.9064\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0177 - val_loss: 0.9299\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0224 - val_loss: 0.9216\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0079 - val_loss: 0.9104\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0112 - val_loss: 0.9890\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0202 - val_loss: 0.9129\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0105 - val_loss: 0.9270\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0165 - val_loss: 0.9305\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0088 - val_loss: 0.9459\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0220 - val_loss: 0.9616\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0126 - val_loss: 0.9157\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0052 - val_loss: 0.9267\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0109 - val_loss: 0.9334\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0107 - val_loss: 0.9265\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0086 - val_loss: 0.9456\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0170 - val_loss: 0.9685\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.0247 - val_loss: 0.9220\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0056 - val_loss: 0.9307\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0060 - val_loss: 0.9468\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0140 - val_loss: 0.9613\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.0081 - val_loss: 0.9485\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0136 - val_loss: 1.0126\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.0140 - val_loss: 0.9428\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0059 - val_loss: 0.9588\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0153 - val_loss: 1.0331\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0130 - val_loss: 0.9511\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0073 - val_loss: 0.9560\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0103 - val_loss: 0.9589\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0101 - val_loss: 0.9502\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0083 - val_loss: 0.9596\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0091 - val_loss: 0.9952\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0142 - val_loss: 0.9661\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0072 - val_loss: 0.9837\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0084 - val_loss: 1.0036\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0129 - val_loss: 0.9712\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0057 - val_loss: 0.9745\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 1s 985us/step - loss: 0.0078 - val_loss: 1.0512\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0246 - val_loss: 0.9723\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 1s 989us/step - loss: 0.0045 - val_loss: 0.9863\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0056 - val_loss: 0.9825\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0061 - val_loss: 0.9825\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.0172 - val_loss: 0.9858\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0081 - val_loss: 0.9825\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0043 - val_loss: 0.9745\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0043 - val_loss: 0.9944\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0189 - val_loss: 0.9791\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.0044 - val_loss: 0.9965\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0046 - val_loss: 1.0274\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0164 - val_loss: 0.9754\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0041 - val_loss: 0.9984\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0040 - val_loss: 1.0205\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0138 - val_loss: 1.0112\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0090 - val_loss: 0.9850\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0044 - val_loss: 0.9936\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.0068 - val_loss: 0.9967\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0141 - val_loss: 0.9972\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0055 - val_loss: 0.9927\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0062 - val_loss: 1.0335\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 1s 989us/step - loss: 0.0103 - val_loss: 1.0172\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 1s 982us/step - loss: 0.0123 - val_loss: 1.0215\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.0055 - val_loss: 1.0114\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 1s 986us/step - loss: 0.0032 - val_loss: 1.0167\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0087 - val_loss: 1.0170\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0090 - val_loss: 1.0127\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0034 - val_loss: 1.0213\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.0064 - val_loss: 1.0329\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0054 - val_loss: 1.0219\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0082 - val_loss: 1.0402\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 1s 985us/step - loss: 0.0060 - val_loss: 1.0408\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0090 - val_loss: 1.0660\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0088 - val_loss: 1.0402\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 1s 983us/step - loss: 0.0040 - val_loss: 1.0293\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.0044 - val_loss: 1.0390\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0087 - val_loss: 1.0419\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0035 - val_loss: 1.0270\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 1s 989us/step - loss: 0.0051 - val_loss: 1.0774\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0102 - val_loss: 1.0237\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 1s 988us/step - loss: 0.0042 - val_loss: 1.0350\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.0026 - val_loss: 1.0444\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 1s 987us/step - loss: 0.0039 - val_loss: 1.0570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2e13f20278>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,batch_size=64,epochs=200,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, len(french_chars)))\n",
    "    target_seq[0, 0,0] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    translated_sequence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        translated_sequence += sampled_char\n",
    "\n",
    "     \n",
    "        if (sampled_char == '\\n' or\n",
    "           len(translated_sequence) >  max([len(txt) for txt in target_texts])):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1,len(french_chars)))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return translated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English text: ['new', 'jersey', 'is', 'sometimes', 'quiet', 'during', 'autumn', ',', 'and', 'it', 'is', 'snowy', 'in', 'april', '.']\n",
      "Translated in French: estparfoiscalmependant\n",
      "English text: ['the', 'united', 'states', 'is', 'usually', 'chilly', 'during', 'july', ',', 'and', 'it', 'is', 'usually', 'freezing', 'in', 'november', '.']\n",
      "Translated in French: estgénéralementgénéralement\n",
      "English text: ['california', 'is', 'usually', 'quiet', 'during', 'march', ',', 'and', 'it', 'is', 'usually', 'hot', 'in', 'june', '.']\n",
      "Translated in French: estgénéralementcalmeau\n",
      "English text: ['the', 'united', 'states', 'is', 'sometimes', 'mild', 'during', 'june', ',', 'and', 'it', 'is', 'cold', 'in', 'september', '.']\n",
      "Translated in French: estparfoisparfoischaud\n",
      "English text: ['your', 'least', 'liked', 'fruit', 'is', 'the', 'grape', ',', 'but', 'my', 'least', 'liked', 'is', 'the', 'apple', '.']\n",
      "Translated in French: moinsaiméfruitestleraisin\n",
      "English text: ['his', 'favorite', 'fruit', 'is', 'the', 'orange', ',', 'but', 'my', 'favorite', 'is', 'the', 'grape', '.']\n",
      "Translated in French: fruitpréféréestl'orange\n",
      "English text: ['paris', 'is', 'relaxing', 'during', 'december', ',', 'but', 'it', 'is', 'usually', 'chilly', 'in', 'july', '.']\n",
      "Translated in French: estrelaxantendécembre,\n",
      "English text: ['new', 'jersey', 'is', 'busy', 'during', 'spring', ',', 'and', 'it', 'is', 'never', 'hot', 'in', 'march', '.']\n",
      "Translated in French: estoccupéauprintemps,et\n",
      "English text: ['our', 'least', 'liked', 'fruit', 'is', 'the', 'lemon', ',', 'but', 'my', 'least', 'liked', 'is', 'the', 'grape', '.']\n",
      "Translated in French: pamplemousseestvotrefruit\n",
      "English text: ['the', 'united', 'states', 'is', 'sometimes', 'busy', 'during', 'january', ',', 'and', 'it', 'is', 'sometimes', 'warm', 'in', 'november', '.']\n",
      "Translated in French: estestparfoischaudenjanvier\n"
     ]
    }
   ],
   "source": [
    "for index in range(10):\n",
    "    input_seq = encoder_input_data[index: index + 1]\n",
    "    translated_sequence = decode_seq(input_seq)\n",
    "\n",
    "    print('English text:', english_texts[index])\n",
    "    print('Translated in French:', translated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
